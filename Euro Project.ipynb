{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction. \n",
    "\n",
    "This notebook is a sample of some basic data science work we might perform for commerical reasons. The dataset today comes from the European Social Survey of 2016/2017 (https://www.kaggle.com/pascalbliem/european-social-survey-ess-8-ed21-201617). \n",
    "The goal is to try and guess country of residence (of which there are 23)  from individual responses on the survey.\n",
    "\n",
    "\n",
    "This dataset was chosen for the demo because it was was relatively small but with many features. The goal of country classification also goes nicely with how people understand the world, especially today where global immigration and nationalism and constantly discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T00:31:27.535362Z",
     "start_time": "2019-11-23T00:31:27.532360Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data \n",
    "\n",
    "Here we read in the csv files as pandas dataframes. Normally, we would do much more expoloratory analysis and visualizations to make sure we know what we are doing, but I've skipped over that part for the sake of time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T00:27:11.094173Z",
     "start_time": "2019-11-23T00:27:09.440242Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (164) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# reading in the data\n",
    "main_df = pd.read_csv(\"ESS8e02.1_F1.csv\")\n",
    "var_df = pd.read_csv(\"variables.csv\")\n",
    "\n",
    "\n",
    " # convenience dict for going between the abreviations and the full country names. Just a good thing to have on hand.\n",
    "country_name_dict = {\"AT\": \"Austria\",                   \n",
    "                     \"BE\": \"Belgium\", \n",
    "                     \"CH\": \"Switzerland\", \n",
    "                     \"CZ\": \"Czech Republic\", \n",
    "                    \"DE\": \"Germany\", \n",
    "                    \"EE\": \"Estonia\", \n",
    "                    \"ES\": \"Spain\", \n",
    "                    \"FI\": \"Finland\", \n",
    "                    \"FR\": \"France\", \n",
    "                    \"GB\": \"United Kingdom\", \n",
    "                    \"HU\": \"Hungary\", \n",
    "                    \"IE\": \"Ireland\", \n",
    "                    \"IL\": \"Israel\", \n",
    "                    \"IS\": \"Iceland\", \n",
    "                    \"IT\": \"Italy\", \n",
    "                    \"LT\": \"Lithuania\", \n",
    "                    \"NL\": \"Netherlands\", \n",
    "                    \"NO\": \"Norway\", \n",
    "                    \"PL\": \"Poland\", \n",
    "                    \"PT\": \"Portugal\", \n",
    "                    \"RU\": \"Russia\", \n",
    "                    \"SE\": \"Sweden\", \n",
    "                    \"SI\": \"Slovenia\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dataset comes in a pretty typical format: there's one csv file filled with the actual data (main_df), and then there is a separate csv file (var_df) explaining what everything means while also having some metadata. The second file is incrediblly useful for feature selection: often times, it is better to focus on the right parts of the dataset rather than just focusing on the algorithm. \n",
    "\n",
    "As mentioned earlier, the european survey file is actually pretty small (less than 45,000 respondents) but it has 534 features per overall. 534 features is a lot. To pare things down somewhat, I decided to drop all columns that are countyr specific. I then went throught the var_df file manually and picked out some features that seemed the most promising. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T00:27:12.596404Z",
     "start_time": "2019-11-23T00:27:12.520596Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# We have to drop all columns from main_df that are country specific. \n",
    "question_labels = list(var_df[\"Name\"])\n",
    "country_specific = list(var_df[\"Country_specific\"])\n",
    "\n",
    "\n",
    "# crude loop to get the column indices to be droped\n",
    "drop_list = []\n",
    "for i in range(len(question_labels)): \n",
    "    if country_specific[i] == \"yes\": \n",
    "        drop_list.append(question_labels[i])\n",
    "        \n",
    "                \n",
    "# Dropping from the main table\n",
    "main_df.drop(labels = drop_list, axis = 1, inplace = True)\n",
    "\n",
    "# and dropping from var_df. \n",
    "var_df.set_index(\"Name\", inplace= True)\n",
    "var_df.drop(labels = drop_list, inplace= True)\n",
    "var_df.reset_index(inplace= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection.\n",
    "\n",
    "\n",
    "After going through var_df I decided on the following features to feed into the model, which are listed below. To the right are the abreviated names along with any codes that indicate missing/incomplete responses.  \n",
    "\n",
    "As one would expect, number 7 (language spoken) is by far the most useful for predicting nationality. This makes sense. If you speak Norwegian on a daily basis ... you probably live in Norway. The others might have some predictive power, but probably to as lesser extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) age/gender                                                                      agea (999), gndr (9)\n",
    "\n",
    "# 2) citizen/born in the country                                                    ctzcntr(9),  brncntr(9)\n",
    "\n",
    "# 3) education, paid work, last 7 days.                                             pdwrk,  edctn                            \n",
    "\n",
    "# 4) Respondent or household member victim of burglary/ assault last 5 years       crmvct (7, 8, 9)\n",
    "\n",
    "# 5) whether they voted in the last national election.                             vote (3, 7, 8, 9)          \n",
    "\n",
    "# 6) daily internet use, in minutes.                                               netustm (6666, 7777, 8888, 9999)\n",
    "\n",
    "# 7**) language spoken, first or second                                             lnghom1, lnghom2  (777, 888, 999)\n",
    "\n",
    "\n",
    "# 8) Would vote  to remain member of European Union or leave                         vteurmmb (33, 44, 55, 65, 77, 88, 99)\n",
    "\n",
    "\n",
    "# 9) ever been divorced                                                             dvrcdeva (7, 8, 9)\n",
    "  \n",
    "# 10) Improve knowledge/skills: course/lecture/conference, last 12 months              atncrse(7, 8, 9)\n",
    "\n",
    "\n",
    "# 11) Years of full-time education completed                                           eduyrs (77, 88, 99)\n",
    "\n",
    "# 12) Year in which last held a paid job                                                    pdjobyr(6666, 7777, 8888, 9999)\n",
    "\n",
    "# 13) News about politics and current affairs, watching, reading or listening, in minutes     nwspol (7777, 8888, 9999)\n",
    "\n",
    "\n",
    "# 14) Would vote to become member of European Union or remain outside         vteubcmb(33, 44, 55, 65, 77, 88, 99)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down below is the block where we replace all of the missing/incompete codes with np.nan for standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T00:27:17.767547Z",
     "start_time": "2019-11-23T00:27:17.746570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tedious replacement of invalid values with np.NAN...Not all of this is neccesary, so feel free to skip this step depending\n",
    "# on the model you want to run. \n",
    "\n",
    "main_df[\"agea\"].replace(999, np.nan)\n",
    "\n",
    "main_df[\"gndr\"].replace(9, np.nan)\n",
    "\n",
    "main_df[\"ctzcntr\"].replace(9, np.nan)\n",
    "\n",
    "main_df[\"brncntr\"].replace(9, np.nan)\n",
    "\n",
    "main_df[\"crmvct\"].replace([7, 8, 9], np.nan)\n",
    "\n",
    "main_df[\"vote\"].replace([3, 7, 8, 9], np.nan)\n",
    "\n",
    "main_df[\"netustm\"].replace([3, 7, 8, 9], np.nan)\n",
    "\n",
    "main_df[\"vteurmmb\"].replace([33, 44, 55, 65, 77, 88, 99], np.nan)\n",
    "\n",
    "main_df[\"vteubcmb\"].replace([33, 44, 55, 65, 77, 88, 99], np.nan)\n",
    "\n",
    "\n",
    "\n",
    "# language ones \n",
    "main_df[\"lnghom1\"].replace([777, 888, 999], np.nan)\n",
    "\n",
    "main_df[\"lnghom2\"].replace([777, 888, 999], np.nan)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling. \n",
    "\n",
    "\n",
    "Now we get to the fun part where we actually apply some machine learning algorithms. For this demo we are going to be using the k-nearest neighbor algorithm along with the random forest algorithm. Also Spoiler alert, k-nearest neighbors is not going to do very well, but it demonstrates the importance of choosing the right approach.\n",
    "\n",
    "\n",
    "An important consideration is that the random forest and k-nearest neighbors algorithms only understand numbers, so we have to convert string labels (such as \"GER\" or \"AT\") into numbers. To do this we will use skleans label encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T00:27:23.198707Z",
     "start_time": "2019-11-23T00:27:23.168786Z"
    }
   },
   "outputs": [],
   "source": [
    "le_language = LabelEncoder()\n",
    "le_country = LabelEncoder()\n",
    "\n",
    "le_language.fit(main_df[\"lnghom1\"])\n",
    "main_df[\"lnghom1\"] = le_language.transform(main_df[\"lnghom1\"])\n",
    "\n",
    "le_language.fit(main_df[\"lnghom2\"])\n",
    "main_df[\"lnghom2\"] = le_language.transform(main_df[\"lnghom2\"])\n",
    "\n",
    "# And now for the country coulmn\n",
    "le_country.fit(main_df[\"cntry\"])\n",
    "main_df[\"cntry\"] = le_country.transform(main_df[\"cntry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our preprocessing is finally done we can train the models with the selected features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T00:28:00.398207Z",
     "start_time": "2019-11-23T00:27:34.873533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy for knn: 0.6666478920836971\n",
      "test accuracy for knn: 0.39851317864383873\n",
      "\n",
      "\n",
      "train accuracy for random forest: 0.9752738742290686\n",
      "test accuracy for random forest: 0.9109033566118495\n"
     ]
    }
   ],
   "source": [
    "# Actual feautures we will use\n",
    "keep_cols = [\"agea\", \"gndr\", \"ctzcntr\", \"brncntr\", \"pdwrk\", \"edctn\", \"crmvct\", \"netustm\", \"vteurmmb\",\n",
    "             \"dvrcdeva\", \"atncrse\", \"eduyrs\", \"pdjobyr\", \"nwspol\", \"lnghom1\", \"lnghom2\", \"vteubcmb\"]\n",
    "\n",
    "# the target vector\n",
    "target_vector = main_df[\"cntry\"]\n",
    "\n",
    "\n",
    "# keep only the columns that we want. \n",
    "feature_df = main_df.copy()\n",
    "feature_df = feature_df.replace(np.nan, -1)\n",
    "feature_df = feature_df[keep_cols]\n",
    "\n",
    "\n",
    "\n",
    "# splitting the data into train and test. Standard 80/20 split.  \n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, target_vector, test_size=0.2, random_state=19)\n",
    "\n",
    "\n",
    "# nearest neighbors classifier. \n",
    "nn_clf = KNeighborsClassifier(n_neighbors= 3)\n",
    "\n",
    "nn_clf.fit(X_train, y_train)\n",
    "\n",
    "nn_y_hat_train = nn_clf.predict(X_train)\n",
    "nn_y_hat_test = nn_clf.predict(X_test)\n",
    "\n",
    "print(\"train accuracy for knn:\", np.mean(y_train == nn_y_hat_train))\n",
    "print(\"test accuracy for knn:\", np.mean(y_test == nn_y_hat_test))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Making our randoom forest and choosing the hyperparameters.\n",
    "rand_forest_clf = RandomForestClassifier(criterion= \"entropy\", \n",
    "                                         n_estimators= 500, max_depth = 20, min_samples_split = 5, min_samples_leaf = 2)\n",
    "\n",
    "\n",
    "rand_forest_clf.fit(X_train, y_train)\n",
    "\n",
    "rand_forest_y_hat_train = rand_forest_clf.predict(X_train)\n",
    "rand_forest_y_hat_test = rand_forest_clf.predict(X_test)\n",
    "\n",
    "print(\"train accuracy for random forest:\", np.mean(y_train == rand_forest_y_hat_train))\n",
    "print(\"test accuracy for random forest:\", np.mean(y_test == rand_forest_y_hat_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifier is clearly superior for this task. This is because k-nearest neighbors only works for euclidean spaces, which is definitely not the case with the features we selected (most notably language, where the mapping is completely arbitrary). \n",
    "\n",
    " One major advantage of random forests (and other tree-based algorithms) is that they are highly interpretable. \n",
    " Below we can can clearly see which features are valued most highly by the random forest algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T00:28:01.538446Z",
     "start_time": "2019-11-23T00:28:00.727792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance: \n",
      "\n",
      "agea 0.024777336101165795\n",
      "gndr 0.005261927093398343\n",
      "ctzcntr 0.006737325337363704\n",
      "brncntr 0.010888378427186875\n",
      "pdwrk 0.0038332466041030517\n",
      "edctn 0.002403172906963358\n",
      "crmvct 0.003993131609881037\n",
      "netustm 0.01866900095319973\n",
      "vteurmmb 0.1540387640344755\n",
      "dvrcdeva 0.003741303641734748\n",
      "atncrse 0.0081725942370872\n",
      "eduyrs 0.03463595339124655\n",
      "pdjobyr 0.011676749301470349\n",
      "nwspol 0.022851144609327485\n",
      "lnghom1 0.5212071084088408\n",
      "lnghom2 0.06817510467571278\n",
      "vteubcmb 0.09893775866684312\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Importance: \\n\")\n",
    "for i in range(len(rand_forest_clf.feature_importances_)): \n",
    "    print(keep_cols[i], rand_forest_clf.feature_importances_[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, by far, the most predictive features are first language, followed by pro-EU sentiment (vteubcmb and vteurmmb). This implies are lot of basic demographic and lifestyle questions are pretty useless and that political sentiment is the better predictor, which I would not have guessed starting out.   \n",
    "\n",
    "\n",
    "Another good approach for this problem is a naive bayes classifier. This is a little tricky though, as the sklearn implmentation we will be using only accepts binary features (such as 1/0 or yes/no), not categorical ones. To get around this, we will use one-hot encoding to convert some categorical features to binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T00:30:34.956054Z",
     "start_time": "2019-11-23T00:30:34.752667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy for BernoulliNB: 0.9108395054774846\n",
      "test accuracy for BernoulliNB: 0.9061725613877\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keep_cols = [\"vteurmmb\", \"vteubcmb\", \"lnghom1\", \"lnghom2\", \"gndr\"]\n",
    "\n",
    "target_vector = main_df[\"cntry\"]\n",
    "\n",
    "\n",
    "# keep only the columns that we want. \n",
    "feature_df = main_df.copy()\n",
    "feature_df = feature_df.replace(np.nan, -1)\n",
    "feature_df = feature_df[keep_cols]\n",
    "\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(categories='auto')\n",
    "\n",
    "ohe.fit(feature_df)\n",
    "feature_df = ohe.transform(feature_df)\n",
    "\n",
    "\n",
    "\n",
    "# splitting the data into train and test. \n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, target_vector, test_size=0.2, random_state=19)\n",
    "\n",
    "\n",
    "\n",
    "nb_classifier = BernoulliNB()\n",
    "\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "nb_y_hat_train = nb_classifier.predict(X_train)\n",
    "nb_y_hat_test = nb_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"train accuracy for BernoulliNB:\", np.mean(y_train == nb_y_hat_train))\n",
    "print(\"test accuracy for BernoulliNB:\", np.mean(y_test == nb_y_hat_test))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are slightly dispointing as the naive bayes actually does slightly worse than a decision tree. \n",
    "Let's make some code to get a better look at the classification error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T00:42:20.909524Z",
     "start_time": "2019-11-23T00:42:20.873621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austria mistaken for Germany on 1301 occasions\n",
      "Austria mistaken for Italy on 1 occasions\n",
      "Austria mistaken for Belgium on 8 occasions\n",
      "Austria mistaken for Hungary on 6 occasions\n",
      "Austria mistaken for Slovenia on 1 occasions\n",
      "Austria mistaken for Spain on 3 occasions\n",
      "Austria mistaken for Ireland on 1 occasions\n",
      "Austria mistaken for France on 1 occasions\n",
      "Austria mistaken for Czech Republic on 1 occasions\n",
      "Austria mistaken for Lithuania on 1 occasions\n",
      "Belgium mistaken for Netherlands on 819 occasions\n",
      "Belgium mistaken for France on 575 occasions\n",
      "Belgium mistaken for Ireland on 24 occasions\n",
      "Belgium mistaken for Poland on 3 occasions\n",
      "Belgium mistaken for Italy on 2 occasions\n",
      "Belgium mistaken for Portugal on 3 occasions\n",
      "Belgium mistaken for Germany on 2 occasions\n",
      "Belgium mistaken for Austria on 3 occasions\n",
      "Belgium mistaken for Spain on 5 occasions\n",
      "Belgium mistaken for Hungary on 1 occasions\n",
      "Switzerland mistaken for Israel on 27 occasions\n",
      "Switzerland mistaken for Russia on 2 occasions\n",
      "Czech Republic mistaken for Ireland on 6 occasions\n",
      "Czech Republic mistaken for Lithuania on 3 occasions\n",
      "Czech Republic mistaken for Finland on 2 occasions\n",
      "Czech Republic mistaken for Germany on 1 occasions\n",
      "Czech Republic mistaken for Belgium on 1 occasions\n",
      "Czech Republic mistaken for Poland on 1 occasions\n",
      "Germany mistaken for Austria on 345 occasions\n",
      "Germany mistaken for Ireland on 9 occasions\n",
      "Germany mistaken for Spain on 6 occasions\n",
      "Germany mistaken for Belgium on 8 occasions\n",
      "Germany mistaken for Italy on 7 occasions\n",
      "Germany mistaken for Portugal on 4 occasions\n",
      "Germany mistaken for Netherlands on 2 occasions\n",
      "Germany mistaken for Poland on 8 occasions\n",
      "Germany mistaken for Lithuania on 4 occasions\n",
      "Germany mistaken for France on 5 occasions\n",
      "Germany mistaken for Sweden on 1 occasions\n",
      "Estonia mistaken for United Kingdom on 4 occasions\n",
      "Spain mistaken for Belgium on 8 occasions\n",
      "Spain mistaken for Ireland on 4 occasions\n",
      "Spain mistaken for Portugal on 3 occasions\n",
      "Spain mistaken for Lithuania on 3 occasions\n",
      "Spain mistaken for Germany on 4 occasions\n",
      "Spain mistaken for France on 2 occasions\n",
      "Spain mistaken for Italy on 4 occasions\n",
      "Finland mistaken for Spain on 2 occasions\n",
      "Finland mistaken for Ireland on 10 occasions\n",
      "Finland mistaken for Sweden on 30 occasions\n",
      "Finland mistaken for Lithuania on 4 occasions\n",
      "Finland mistaken for Germany on 2 occasions\n",
      "Finland mistaken for Belgium on 5 occasions\n",
      "Finland mistaken for Italy on 2 occasions\n",
      "France mistaken for Belgium on 39 occasions\n",
      "France mistaken for Germany on 4 occasions\n",
      "France mistaken for Ireland on 4 occasions\n",
      "France mistaken for Portugal on 7 occasions\n",
      "France mistaken for Italy on 1 occasions\n",
      "France mistaken for Spain on 4 occasions\n",
      "United Kingdom mistaken for France on 1 occasions\n",
      "United Kingdom mistaken for Estonia on 22 occasions\n",
      "Hungary mistaken for Ireland on 4 occasions\n",
      "Hungary mistaken for Italy on 2 occasions\n",
      "Hungary mistaken for Austria on 1 occasions\n",
      "Hungary mistaken for Germany on 15 occasions\n",
      "Ireland mistaken for Italy on 3 occasions\n",
      "Ireland mistaken for Poland on 58 occasions\n",
      "Ireland mistaken for Germany on 8 occasions\n",
      "Ireland mistaken for France on 13 occasions\n",
      "Ireland mistaken for Lithuania on 10 occasions\n",
      "Ireland mistaken for Sweden on 9 occasions\n",
      "Ireland mistaken for Spain on 2 occasions\n",
      "Ireland mistaken for Portugal on 6 occasions\n",
      "Israel mistaken for Russia on 66 occasions\n",
      "Israel mistaken for Switzerland on 25 occasions\n",
      "Iceland mistaken for Switzerland on 18 occasions\n",
      "Iceland mistaken for Israel on 10 occasions\n",
      "Iceland mistaken for Russia on 1 occasions\n",
      "Italy mistaken for Belgium on 28 occasions\n",
      "Italy mistaken for Spain on 13 occasions\n",
      "Italy mistaken for Ireland on 10 occasions\n",
      "Italy mistaken for Sweden on 5 occasions\n",
      "Italy mistaken for France on 6 occasions\n",
      "Italy mistaken for Germany on 3 occasions\n",
      "Italy mistaken for Austria on 1 occasions\n",
      "Lithuania mistaken for Italy on 5 occasions\n",
      "Lithuania mistaken for Poland on 65 occasions\n",
      "Lithuania mistaken for Ireland on 6 occasions\n",
      "Netherlands mistaken for Belgium on 65 occasions\n",
      "Netherlands mistaken for Sweden on 2 occasions\n",
      "Netherlands mistaken for Lithuania on 7 occasions\n",
      "Netherlands mistaken for Ireland on 13 occasions\n",
      "Netherlands mistaken for Germany on 5 occasions\n",
      "Netherlands mistaken for Austria on 2 occasions\n",
      "Netherlands mistaken for Spain on 3 occasions\n",
      "Netherlands mistaken for Italy on 2 occasions\n",
      "Netherlands mistaken for Portugal on 2 occasions\n",
      "Netherlands mistaken for Hungary on 1 occasions\n",
      "Netherlands mistaken for Poland on 1 occasions\n",
      "Norway mistaken for Switzerland on 5 occasions\n",
      "Norway mistaken for Israel on 1 occasions\n",
      "Poland mistaken for Ireland on 4 occasions\n",
      "Poland mistaken for Belgium on 2 occasions\n",
      "Poland mistaken for Germany on 1 occasions\n",
      "Poland mistaken for Lithuania on 3 occasions\n",
      "Poland mistaken for Sweden on 2 occasions\n",
      "Portugal mistaken for Ireland on 9 occasions\n",
      "Portugal mistaken for Belgium on 1 occasions\n",
      "Portugal mistaken for France on 1 occasions\n",
      "Portugal mistaken for Lithuania on 1 occasions\n",
      "Portugal mistaken for Spain on 1 occasions\n",
      "Russia mistaken for Israel on 17 occasions\n",
      "Russia mistaken for Switzerland on 9 occasions\n",
      "Sweden mistaken for Ireland on 16 occasions\n",
      "Sweden mistaken for Finland on 2 occasions\n",
      "Sweden mistaken for Spain on 9 occasions\n",
      "Sweden mistaken for Italy on 4 occasions\n",
      "Sweden mistaken for Belgium on 6 occasions\n",
      "Sweden mistaken for Netherlands on 1 occasions\n",
      "Sweden mistaken for Germany on 5 occasions\n",
      "Sweden mistaken for France on 2 occasions\n",
      "Sweden mistaken for Hungary on 1 occasions\n",
      "Slovenia mistaken for Italy on 2 occasions\n",
      "Slovenia mistaken for Austria on 13 occasions\n",
      "Slovenia mistaken for Lithuania on 1 occasions\n",
      "Slovenia mistaken for Belgium on 3 occasions\n",
      "Slovenia mistaken for Hungary on 3 occasions\n",
      "Slovenia mistaken for Germany on 4 occasions\n",
      "Slovenia mistaken for Sweden on 2 occasions\n"
     ]
    }
   ],
   "source": [
    "# Make some code to look at classification error\n",
    "\n",
    "predicted_labels = nb_classifier.predict(feature_df)\n",
    "\n",
    "# a default dict that default key of 0 (int)\n",
    "misclass_dict = defaultdict(int)\n",
    "\n",
    "#  We have to convert them back from numbers to the string abbreviations. Bit of a hassle \n",
    "main_df[\"cntry\"] = le_country.inverse_transform(main_df[\"cntry\"])\n",
    "predicted_labels = le_country.inverse_transform(predicted_labels)\n",
    "\n",
    "\n",
    "for count, cntry_name in enumerate(main_df[\"cntry\"]): \n",
    "    \n",
    "    # If there is a classification error..\n",
    "    if cntry_name != predicted_labels[count]:\n",
    "        \n",
    "        # add it to the default dict and increment.\n",
    "        misclass_dict[(cntry_name, predicted_labels[count])] += 1\n",
    "        \n",
    "        \n",
    "main_df[\"cntry\"] = le_country.transform(main_df[\"cntry\"])\n",
    "               \n",
    "\n",
    "for error in misclass_dict:\n",
    "    print(\"{} mistaken for {} on {} occasions\".format(country_name_dict[error[0]], country_name_dict[error[1]], \n",
    "                                                      misclass_dict[error]))     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the classifier has a lot of trouble separating Germany/Austria, Belgium/Netherlands/France, and Russia/Estonia. This is because these countries have a lot of language overlap, and in these cases the classifier usually assigns them to whichever country is seen more in the dataset (as it should). \n",
    "\n",
    "Next, we will try to improve our results by turning the naive bayes classifier output into a new feature for our dataset. This is known as \"boosting\", and it is a key technique in data science. The hope is that since the naive bayes classifier output correlates highly with the country, the random forest can take that information into account when classifying based on the other features. Think of it as a very strong \"hint\" of how that datapoint should be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df[\"nb_output\"] = nb_classifier.predict(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T01:01:36.501591Z",
     "start_time": "2019-11-23T01:01:18.861101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy for random forest: 0.9656706750401307\n",
      "test accuracy for random forest: 0.9201396710970939\n"
     ]
    }
   ],
   "source": [
    "# Same keep cols as before, only that we have \n",
    "keep_cols = [\"agea\", \"ctzcntr\", \"brncntr\", \"pdwrk\", \"edctn\", \"crmvct\", \"netustm\", \"vteurmmb\",\n",
    "             \"dvrcdeva\", \"atncrse\", \"eduyrs\", \"pdjobyr\", \"nwspol\", \"lnghom1\", \"lnghom2\", \"vteubcmb\",  \n",
    "             \"nb_output\"]\n",
    "\n",
    "\n",
    "# Redoing all the random forest stuff from before. This is a litteral copy pase \n",
    "\n",
    "target_vector = main_df[\"cntry\"]\n",
    "\n",
    "\n",
    "# keep only the columns that we want. \n",
    "feature_df = main_df.copy()\n",
    "feature_df = feature_df.replace(np.nan, -1)\n",
    "feature_df = feature_df[keep_cols]\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, target_vector, test_size=0.2, random_state=19)\n",
    "\n",
    "\n",
    "\n",
    "# Making our randoom forest and choosing the hyperparameters.\n",
    "rand_forest_clf = RandomForestClassifier(criterion= \"entropy\", \n",
    "                                         n_estimators= 500, max_depth = 20, min_samples_split = 5, min_samples_leaf = 2)\n",
    "\n",
    "\n",
    "rand_forest_clf.fit(X_train, y_train)\n",
    "\n",
    "rand_forest_y_hat_train = rand_forest_clf.predict(X_train)\n",
    "rand_forest_y_hat_test = rand_forest_clf.predict(X_test)\n",
    "\n",
    "print(\"train accuracy for random forest:\", np.mean(y_train == rand_forest_y_hat_train))\n",
    "print(\"test accuracy for random forest:\", np.mean(y_test == rand_forest_y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did slightly better. Yay. \n",
    "\n",
    "\n",
    "If we were going to continue this project, we would investigate the main features that separate our main clusters of confusion (such as Germany/Austria). To accomplish this we would do more exploratory analysis, going feature by feature and making visualizations. We might also look into doing more advanced feature engineering/feature selection by doing a PCA.\n",
    "\n",
    "Overall, this project could progress in many different ways and there is room for several different approaches.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "358px",
    "left": "529.364px",
    "right": "20px",
    "top": "38px",
    "width": "601px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
